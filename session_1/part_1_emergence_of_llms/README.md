# The Emergence of LLMs


```mermaid
timeline
        title Brief History of NLP
        section 1967
          MITâ€™s Eliza (First Chatbot) : ğŸ‘ŒğŸ¼ Groundbreaking human-computer interaction : ğŸ‘ğŸ¼ Limited contextual understanding
        section 1986
          Recurrent Neural Networks (RNNs) : ğŸ‘ŒğŸ¼ Memory for sequences : ğŸ‘ğŸ¼ Vanishing gradient for long sentences
        section 1997
          Long Short-Term Memory (LSTM) : ğŸ‘ŒğŸ¼ Selective ability to memorize or forget, retained long-term dependencies : ğŸ‘ğŸ¼ Complexity due to 3 different gates
        section 2014
          Gated Recurrent Units (GRUs) : ğŸ‘ŒğŸ¼ Simplified gating, efficient using reset and update gates : ğŸ‘ğŸ¼ Limited contextual understanding for long sequences
          Attention Mechanism: ğŸ‘ŒğŸ¼ Dynamic sequence processing, better context retention, offered fresh perspective : ğŸ‘ğŸ¼ Increased computational complexity
        section 2017
          Transformer Architecture : ğŸ‘ŒğŸ¼ Parallel sequence processing through multi-head attention : ğŸ‘ğŸ¼ High computational demand. Due to their size and complexity
```


```mermaid
timeline
        title Building Upon The Transformers
        section 2018
          OpenAIâ€™s GPT-1, Google's BERT Model : ğŸ‘ŒğŸ¼ Bert - bidirectional encoder only <br>  GPT - unidirectional, decoder only : ğŸ‘ğŸ¼ Requires task specific fine-tuning
        title Building Upon The Transformers
        section 2019
          OpenAI's GPT-2, Googleâ€™s T5 : ğŸ‘ŒğŸ¼ Multi task solving, massive amount of compressed knowledge e.g. GPT-2 (40B data), T5 (7TB data) : ğŸ‘ğŸ¼ Model size, training complexity
        section 2020
          OpenAI's GPT-3 : ğŸ‘ŒğŸ¼ Unprecedented versatility, Few shot learning : ğŸ‘ğŸ¼ Enormous computational requirements, ethical concerns
        section 2022
          OpenAI's InstructGPT : ğŸ‘ŒğŸ¼ Learn from human feedback during training to follow human instructions better : ğŸ‘ğŸ¼ Tailored for instructions oriented tasks. Not suitable for natural, dynamic conversation
          ChatGPT : ğŸ‘ŒğŸ¼ Sibling of InstructGPT, optimized for conversations : ğŸ‘ğŸ¼ Works only with textual data, prone to hallucination, limited knowledge of world upto 2022
        section 2023
          GPT-4 : ğŸ‘ŒğŸ¼ Handles both text and image, human level on various benchmarks, allows integration of external tools such as web-browsing and code-interpreter : ğŸ‘ğŸ¼ Lacks other modalities
```